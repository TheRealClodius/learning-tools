# ExecutionSummarizer Configuration
# Generates intelligent narrative summaries of tool execution results

# System prompt for tool execution summarization
system_prompt: |
  Create a concise, single-line narrative summary of this tool execution:

  Format: ⚡️Used *[tool_name]* to [action]. [Brief result from JSON]

  Examples:
  - weather.current → ⚡️Used *weather* *current* to get weather for London. Currently 18.5°C with broken clouds
  - slack.vector_search → ⚡️Used *slack* *vector_search* to search for 'project status'. Found 12 results
  - memory.retrieve → ⚡️Used *memory* *retrieve* to search for user preferences. Found recent conversations
  - weather.search → ⚡️Used *weather* *search* to find locations matching 'Tokyo'. Found 3 matches

  Write the narrative summary:

# Model configuration for summarization
model_config:
  model: "gemini-2.5-flash"
  temperature: 0.1  # Low temperature for consistent, factual summaries
  max_tokens: 500   # Short, concise summaries
  timeout: 5        # Fast timeout for tool summaries

# Fallback configuration when LLM is unavailable
fallback_config:
  ensure_emoji_prefix: true
  max_result_tokens: 2000  # Truncate long results to token limit for LLM processing
  weather_units_preference: "metric"  # Default to Celsius for weather tools
  
# Tool-specific formatting preferences
tool_formatting:
  weather:
    temperature_unit: "°C"
    include_conditions: true
    include_location: true
  
  memory:
    show_count: true
    mention_type: true  # short_term vs retrieved_pages
  
  slack:
    show_count: true
    include_channels: true
  
  search:
    show_count: true
    truncate_long_queries: 50
